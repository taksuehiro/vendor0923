# apps/app_streamlit.py
from __future__ import annotations
from collections import Counter, defaultdict
from pathlib import Path
import os
import streamlit as st
from dotenv import load_dotenv

from langchain_community.vectorstores import FAISS
from rag_core.bedrock_embeddings import TitanEmbeddings

from rag_core.indexer import load_documents_auto, load_documents_from_dir, build_faiss

load_dotenv()
st.set_page_config(page_title="ãƒ™ãƒ³ãƒ€ãƒ¼æ¤œç´¢ï¼ˆãƒ­ãƒ¼ã‚«ãƒ«RAG, Streamlitï¼‰", page_icon="ğŸ”", layout="wide")
st.markdown("# ğŸ” ãƒ™ãƒ³ãƒ€ãƒ¼æ¤œç´¢ï¼ˆãƒ­ãƒ¼ã‚«ãƒ«RAG, Streamlitï¼‰")

# ãƒ‘ã‚¹è§£æ±ºï¼šapps/ ã‹ã‚‰è¦‹ã¦ ../data ã‚’æ—¢å®š
PROJECT_ROOT = Path(__file__).resolve().parents[1]
DATA_DIR = PROJECT_ROOT / "data"
DEFAULT_JSON = DATA_DIR / "ãƒ™ãƒ³ãƒ€ãƒ¼èª¿æŸ»JSONç·´ç¿’ç‰ˆ.json"
DEFAULT_VS_DIR = PROJECT_ROOT / ".vectorstores" / "vendors_faiss"

with st.sidebar:
    st.subheader("ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿")
    inp = st.text_input("å…¥åŠ›ï¼ˆ.json/.md ã¾ãŸã¯ ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªï¼‰", str(DEFAULT_JSON))
    chunk_size = st.number_input("chunk_sizeï¼ˆJSONã¯é€šå¸¸0ï¼‰", min_value=0, max_value=4000, value=0, step=50)
    chunk_overlap = st.number_input("chunk_overlap", min_value=0, max_value=1000, value=0, step=10)
    do_load = st.button("èª­ã¿è¾¼ã‚€ / å†ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹", use_container_width=True)

def _summarize(docs):
    by_status = Counter([d.metadata.get("status","-") for d in docs])
    by_cat = Counter([d.metadata.get("category","-") for d in docs])
    return by_status, by_cat.most_common(10)

@st.cache_resource(show_spinner=False)
def _emb():
    return TitanEmbeddings()

def _build_or_load_vectorstore(docs):
    DEFAULT_VS_DIR.mkdir(parents=True, exist_ok=True)
    vs = build_faiss(docs, embeddings=_emb(), out_dir=DEFAULT_VS_DIR)
    return vs

if do_load or "docs" not in st.session_state:
    try:
        p = Path(inp).expanduser()
        if p.is_dir():
            docs = load_documents_from_dir(p, chunk_size=chunk_size, chunk_overlap=chunk_overlap)
        else:
            docs = load_documents_auto(p, chunk_size=chunk_size, chunk_overlap=chunk_overlap)
        st.session_state["docs"] = docs
        st.success(f"èª­ã¿è¾¼ã¿æˆåŠŸ: {p}ï¼ˆ{len(docs)} ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆï¼‰")
        st.session_state["vs"] = _build_or_load_vectorstore(docs)
    except Exception as e:
        st.error(f"ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")

docs = st.session_state.get("docs", [])
vs = st.session_state.get("vs", None)

# --- KBãƒ“ãƒ¥ãƒ¼ã‚¢ ---
with st.expander("ğŸ“š KBãƒ“ãƒ¥ãƒ¼ã‚¢ï¼ˆä»¶æ•°ã‚µãƒãƒªãƒ»æ¬ æãƒã‚§ãƒƒã‚¯ãƒ»ä¸€è¦§ï¼‰", expanded=True):
    st.markdown(f"- ç·ä»¶æ•°: **{len(docs)}**")
    by_status, top_cat = _summarize(docs)
    st.markdown("**ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹å†…è¨³**: " + ", ".join([f"{k}: {v}" for k,v in by_status.items()]) or "-")
    st.markdown("**ã‚«ãƒ†ã‚´ãƒªä¸Šä½**: " + ", ".join([f"{k}: {v}" for k,v in top_cat]) or "-")

    # æ¬ æ
    missing = defaultdict(list)
    for i, d in enumerate(docs):
        for key in ("vendor_id","name","category","status"):
            if not d.metadata.get(key):
                missing[key].append(i)
    if missing:
        st.warning("ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿æ¬ æ: " + ", ".join([f"{k}={len(v)}ä»¶" for k,v in missing.items()]))
    else:
        st.info("ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿æ¬ æãªã—")

    # ä¸€è¦§
    for i, d in enumerate(docs[:50]):  # è¡¨ç¤ºä¸Šé™
        st.markdown(f"**#{i+1} {d.metadata.get('vendor_id')} â€” {d.metadata.get('name')}**")
        st.caption(f"{d.metadata.get('category')} / {', '.join(d.metadata.get('industry_tags', [])) or '-'}")
        st.code(d.page_content[:800])

st.divider()

# --- æ¤œç´¢ ---
st.subheader("ğŸ” æ¤œç´¢")
q = st.text_input("ã‚¯ã‚¨ãƒªï¼ˆä¾‹ï¼šå¥‘ç´„æ›¸ ç®¡ç† æ³•å‹™ / ç”»åƒæ¤œæŸ» è£½é€  ãªã©ï¼‰", "")
topk = st.slider("TopK", min_value=1, max_value=20, value=8)
if st.button("æ¤œç´¢ã™ã‚‹", use_container_width=True):
    if not vs:
        st.error("ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢æœªæ§‹ç¯‰ã§ã™ã€‚å·¦ã®ã€èª­ã¿è¾¼ã‚€ / å†ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã€ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")
    else:
        try:
            retriever = vs.as_retriever(search_kwargs={"k": topk})
            results = retriever.get_relevant_documents(q)
            st.markdown(f"**æ¤œç´¢çµæœ: {len(results)} ä»¶ï¼ˆä¸Šä½ {topk}ï¼‰**")
            for i, r in enumerate(results):
                md = r.metadata
                st.markdown(f"**#{i+1} {md.get('vendor_id')} â€” {md.get('name')}**")
                st.caption(f"{md.get('category')} / {', '.join(md.get('industry_tags', [])) or '-'} / {md.get('status')}")
                st.code(r.page_content[:1200])
        except Exception as e:
            st.error(f"æ¤œç´¢ã‚¨ãƒ©ãƒ¼: {e}")


